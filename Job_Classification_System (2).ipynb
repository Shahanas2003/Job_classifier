{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "\n",
        "def scrape_karkidi_jobs(keyword=\"data science\", pages=1):\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    base_url = \"https://www.karkidi.com/Find-Jobs/{page}/all/India?search={query}\"\n",
        "    jobs_list = []\n",
        "\n",
        "    for page in range(1, pages + 1):\n",
        "        url = base_url.format(page=page, query=keyword.replace(' ', '%20'))\n",
        "        print(f\"Scraping page: {page} - {url}\")\n",
        "        response = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        job_blocks = soup.find_all(\"div\", class_=\"ads-details\")\n",
        "        for job in job_blocks:\n",
        "            try:\n",
        "                title = (job.find(\"h4\") or job.find(\"h2\")).get_text(strip=True) if job.find(\"h4\") or job.find(\"h2\") else \"\"\n",
        "                company = \"\"\n",
        "                company_tag = job.find(\"a\", href=lambda x: x and \"Employer-Profile\" in x)\n",
        "                if not company_tag:\n",
        "                    company_tag = job.find(\"span\", class_=\"company-name\")\n",
        "                if company_tag:\n",
        "                    company = company_tag.get_text(strip=True)\n",
        "\n",
        "                location = job.find(\"p\").get_text(strip=True) if job.find(\"p\") else \"\"\n",
        "                experience = job.find(\"p\", class_=\"emp-exp\").get_text(strip=True) if job.find(\"p\", class_=\"emp-exp\") else \"\"\n",
        "                summary = \"\"\n",
        "                skills = \"\"\n",
        "\n",
        "                key_skills_tag = job.find(\"span\", string=\"Key Skills\")\n",
        "                if key_skills_tag:\n",
        "                    skills = key_skills_tag.find_next(\"p\").get_text(strip=True)\n",
        "\n",
        "                summary_tag = job.find(\"span\", string=\"Summary\")\n",
        "                if summary_tag:\n",
        "                    summary = summary_tag.find_next(\"p\").get_text(strip=True)\n",
        "\n",
        "                # Alternative fallback for skills block\n",
        "                if not skills:\n",
        "                    skills_block = job.find(\"div\", class_=\"job-skills\")\n",
        "                    skills = skills_block.get_text(strip=True) if skills_block else \"\"\n",
        "\n",
        "                jobs_list.append({\n",
        "                    \"Title\": title,\n",
        "                    \"Company\": company,\n",
        "                    \"Location\": location,\n",
        "                    \"Experience\": experience,\n",
        "                    \"Summary\": summary,\n",
        "                    \"Skills\": skills\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"Error parsing job block: {e}\")\n",
        "                continue\n",
        "\n",
        "        time.sleep(1)  # Be nice to the server\n",
        "\n",
        "    df = pd.DataFrame(jobs_list)\n",
        "    os.makedirs(\"data\", exist_ok=True)\n",
        "    df.to_csv(\"data/jobs_data.csv\", index=False)\n",
        "    print(f\" Saved {len(df)} jobs to data/jobs_data.csv\")\n",
        "    return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df_jobs = scrape_karkidi_jobs(keyword=\"data science\", pages=2)\n",
        "    print(df_jobs.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gbgo5D2bH3Fz",
        "outputId": "575b40b0-662d-489d-b91d-fbef5c7c76fe"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page: 1 - https://www.karkidi.com/Find-Jobs/1/all/India?search=data%20science\n",
            "Scraping page: 2 - https://www.karkidi.com/Find-Jobs/2/all/India?search=data%20science\n",
            " Saved 20 jobs to data/jobs_data.csv\n",
            "                                               Title         Company  \\\n",
            "0          Machine Learning Physical Design Engineer          Google   \n",
            "1  Staff Software Engineer - Monetization, Poe (R...     Quora, Inc.   \n",
            "2  Staff Backend Engineer - Bot Creator Ecosystem...     Quora, Inc.   \n",
            "3  Senior Backend Engineer - Bot Creator Ecosyste...     Quora, Inc.   \n",
            "4                         Data Scientist Lead - AIML  JPMorgan Chase   \n",
            "\n",
            "                      Location Experience  \\\n",
            "0  Bengaluru, Karnataka, India   4-6 year   \n",
            "1                        India  8-10 year   \n",
            "2                        India  8-10 year   \n",
            "3                        India   6-8 year   \n",
            "4  Bengaluru, Karnataka, India   6-8 year   \n",
            "\n",
            "                                             Summary  \\\n",
            "0  Minimum qualifications:Bachelor's degree in El...   \n",
            "1  About Quora:Quora’s mission is to grow and sha...   \n",
            "2  About Quora:Quora’s mission is to grow and sha...   \n",
            "3  About Quora:Quora’s mission is to grow and sha...   \n",
            "4  We have an opportunity to impact your career a...   \n",
            "\n",
            "                                              Skills  \n",
            "0  Aartificial intelligence,Algorithms,Data struc...  \n",
            "1  Aartificial intelligence,Analytical and Proble...  \n",
            "2  Aartificial intelligence,API,Data science tech...  \n",
            "3  Aartificial intelligence,API,Data science tech...  \n",
            "4  Aartificial intelligence,Data science techniqu...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from joblib import dump\n",
        "import re\n",
        "\n",
        "# Load scraped data\n",
        "df = pd.read_csv(\"data/jobs_data.csv\")\n",
        "\n",
        "# Step 1: Preprocess Skills\n",
        "def clean_skills(skills):\n",
        "    if pd.isna(skills):\n",
        "        return \"\"\n",
        "    skills = skills.lower()\n",
        "    skills = re.sub(r\"[^a-zA-Z0-9, ]\", \"\", skills)  # Remove special characters\n",
        "    skills = [skill.strip() for skill in skills.split(\",\") if skill.strip()]\n",
        "    return \" \".join(skills)  # Return as string for vectorization\n",
        "\n",
        "df[\"Cleaned_Skills\"] = df[\"Skills\"].apply(clean_skills)\n",
        "\n",
        "# Step 2: Vectorize Skills using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df[\"Cleaned_Skills\"])\n",
        "\n",
        "# Step 3: KMeans Clustering\n",
        "n_clusters = 5  # You can experiment with different numbers\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "df[\"Cluster\"] = kmeans.fit_predict(X)\n",
        "\n",
        "# Step 4: Save model and data\n",
        "os.makedirs(\"models\", exist_ok=True) # Create the models directory if it doesn't exist\n",
        "dump(kmeans, \"models/kmeans_model.joblib\")\n",
        "dump(vectorizer, \"models/tfidf_vectorizer.joblib\")\n",
        "df.to_csv(\"data/clustered_jobs.csv\", index=False)"
      ],
      "metadata": {
        "id": "jiqOsB94wRhH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import load\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load saved model and vectorizer\n",
        "kmeans = load(\"models/kmeans_model.joblib\")\n",
        "vectorizer = load(\"models/tfidf_vectorizer.joblib\")\n",
        "\n",
        "# Preprocessing function (same as before)\n",
        "def clean_skills(skills):\n",
        "    if pd.isna(skills):\n",
        "        return \"\"\n",
        "    skills = skills.lower()\n",
        "    skills = re.sub(r\"[^a-zA-Z0-9, ]\", \"\", skills)\n",
        "    skills = [skill.strip() for skill in skills.split(\",\") if skill.strip()]\n",
        "    return \" \".join(skills)\n",
        "\n",
        "def classify_new_jobs(df_new_jobs):\n",
        "    df_new_jobs[\"Cleaned_Skills\"] = df_new_jobs[\"Skills\"].apply(clean_skills)\n",
        "    X_new = vectorizer.transform(df_new_jobs[\"Cleaned_Skills\"])\n",
        "    df_new_jobs[\"Predicted_Cluster\"] = kmeans.predict(X_new)\n",
        "    return df_new_jobs\n"
      ],
      "metadata": {
        "id": "ZdjsHfeGwXIM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def notify_user(df_new_jobs, preferred_cluster):\n",
        "    matched_jobs = df_new_jobs[df_new_jobs[\"Predicted_Cluster\"] == preferred_cluster]\n",
        "\n",
        "    if not matched_jobs.empty:\n",
        "        print(f\" Found {len(matched_jobs)} new job(s) in your preferred category (Cluster {preferred_cluster}):\\n\")\n",
        "        for _, row in matched_jobs.iterrows():\n",
        "            print(f\"- {row['Title']} at {row['Company']}\")\n",
        "    else:\n",
        "        print(f\"No new jobs found in Cluster {preferred_cluster} today.\")\n"
      ],
      "metadata": {
        "id": "dX6btvkfwZsK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume this is your new scraped dataset\n",
        "df_new_jobs = pd.read_csv(\"data/jobs_data.csv\")\n",
        "\n",
        "# Step 1: Classify them using the trained model\n",
        "df_new_classified = classify_new_jobs(df_new_jobs)\n",
        "\n",
        "# Step 2: Notify the user if any match their preferred cluster\n",
        "notify_user(df_new_classified, preferred_cluster=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TE_QXDoswddG",
        "outputId": "2e9fe721-6375-476f-f551-f5507ef01277"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Found 6 new job(s) in your preferred category (Cluster 1):\n",
            "\n",
            "- Staff Software Engineer - Monetization, Poe (Remote) at Quora, Inc.\n",
            "- Staff Backend Engineer - Bot Creator Ecosystem, Poe (Remote) at Quora, Inc.\n",
            "- Senior Backend Engineer - Bot Creator Ecosystem, Poe (Remote) at Quora, Inc.\n",
            "- Staff Software Engineer - Monetization, Poe (Remote) at Quora, Inc.\n",
            "- Staff Backend Engineer - Bot Creator Ecosystem, Poe (Remote) at Quora, Inc.\n",
            "- Senior Backend Engineer - Bot Creator Ecosystem, Poe (Remote) at Quora, Inc.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVgdpR8cxP94",
        "outputId": "342f6440-8066-45a4-9997-592b32081244"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.45.1-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.4)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.13.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.39.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.45.1-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.45.1 watchdog-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 🛠 Create the app.py file inside Colab\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "from joblib import load\n",
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "# Load models once\n",
        "kmeans = load(\"models/kmeans_model.joblib\")\n",
        "vectorizer = load(\"models/tfidf_vectorizer.joblib\")\n",
        "\n",
        "def clean_skills(skills):\n",
        "    if pd.isna(skills):\n",
        "        return \"\"\n",
        "    skills = skills.lower()\n",
        "    skills = re.sub(r\"[^a-zA-Z0-9, ]\", \"\", skills)\n",
        "    skills = [skill.strip() for skill in skills.split(\",\") if skill.strip()]\n",
        "    return \" \".join(skills)\n",
        "\n",
        "# Use your existing scraper logic (simplified here)\n",
        "def scrape_karkidi_jobs(keyword=\"data science\", pages=1):\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    base_url = \"https://www.karkidi.com/Find-Jobs/{page}/all/India?search={query}\"\n",
        "    jobs_list = []\n",
        "\n",
        "    for page in range(1, pages + 1):\n",
        "        url = base_url.format(page=page, query=keyword.replace(' ', '%20'))\n",
        "        response = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        job_blocks = soup.find_all(\"div\", class_=\"ads-details\")\n",
        "\n",
        "        for job in job_blocks:\n",
        "            try:\n",
        "                title = (job.find(\"h4\") or job.find(\"h2\")).get_text(strip=True) if job.find(\"h4\") or job.find(\"h2\") else \"\"\n",
        "                company_tag = job.find(\"a\", href=lambda x: x and \"Employer-Profile\" in x)\n",
        "                company = company_tag.get_text(strip=True) if company_tag else \"\"\n",
        "                skills = \"\"\n",
        "                key_skills_tag = job.find(\"span\", string=\"Key Skills\")\n",
        "                if key_skills_tag:\n",
        "                    skills = key_skills_tag.find_next(\"p\").get_text(strip=True)\n",
        "                if not skills:\n",
        "                    skills_block = job.find(\"div\", class_=\"job-skills\")\n",
        "                    skills = skills_block.get_text(strip=True) if skills_block else \"\"\n",
        "\n",
        "                jobs_list.append({\n",
        "                    \"Title\": title,\n",
        "                    \"Company\": company,\n",
        "                    \"Skills\": skills\n",
        "                })\n",
        "            except:\n",
        "                continue\n",
        "        time.sleep(1)\n",
        "\n",
        "    return pd.DataFrame(jobs_list)\n",
        "\n",
        "def classify_new_jobs(df_new_jobs):\n",
        "    df_new_jobs[\"Cleaned_Skills\"] = df_new_jobs[\"Skills\"].apply(clean_skills)\n",
        "    X_new = vectorizer.transform(df_new_jobs[\"Cleaned_Skills\"])\n",
        "    df_new_jobs[\"Predicted_Cluster\"] = kmeans.predict(X_new)\n",
        "    return df_new_jobs\n",
        "\n",
        "def notify_user(df_new_jobs, preferred_cluster):\n",
        "    matched_jobs = df_new_jobs[df_new_jobs[\"Predicted_Cluster\"] == preferred_cluster]\n",
        "    if not matched_jobs.empty:\n",
        "        return matched_jobs[[\"Title\", \"Company\"]]\n",
        "    else:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# --- Streamlit UI ---\n",
        "st.title(\"Job Posting Classifier and Notifier\")\n",
        "\n",
        "keyword = st.text_input(\"Enter skill keyword(s) to search jobs:\", \"data science\")\n",
        "pages = st.slider(\"Number of pages to scrape:\", 1, 5, 1)\n",
        "\n",
        "if st.button(\"Scrape and Classify Jobs\"):\n",
        "    with st.spinner(\"Scraping jobs...\"):\n",
        "        df_jobs = scrape_karkidi_jobs(keyword, pages)\n",
        "\n",
        "    with st.spinner(\"Classifying jobs...\"):\n",
        "        df_classified = classify_new_jobs(df_jobs)\n",
        "\n",
        "    st.success(f\"Found {len(df_classified)} jobs and classified into clusters.\")\n",
        "\n",
        "    cluster_options = df_classified[\"Predicted_Cluster\"].unique().tolist()\n",
        "    preferred_cluster = st.selectbox(\"Select your preferred cluster:\", cluster_options)\n",
        "\n",
        "    matched_jobs = notify_user(df_classified, preferred_cluster)\n",
        "\n",
        "    if not matched_jobs.empty:\n",
        "        st.markdown(f\"### 🔔 Jobs in Cluster {preferred_cluster} matching your interest:\")\n",
        "        for idx, row in matched_jobs.iterrows():\n",
        "            st.write(f\"**{row['Title']}** at *{row['Company']}*\")\n",
        "    else:\n",
        "        st.write(f\"No new jobs found in Cluster {preferred_cluster}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvLmKocxyoYJ",
        "outputId": "430c9083-aa03-454c-9096-bf3fdac824b1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}