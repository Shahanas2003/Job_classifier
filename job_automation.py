# -*- coding: utf-8 -*-
"""Job_automation.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/197jn0x5sI3KiCo_OH8WAvdxhsoKCtmxU
"""

!pip install apscheduler

import pandas as pd
from joblib import load
import re
import requests
from bs4 import BeautifulSoup

# Load trained models
kmeans = load("models/kmeans_model.joblib")
vectorizer = load("models/tfidf_vectorizer.joblib")

# Clean skills text
def clean_skills(skills):
    if pd.isna(skills):
        return ""
    skills = re.sub(r"[^a-zA-Z0-9, ]", "", skills.lower())
    return " ".join(skill.strip() for skill in skills.split(",") if skill.strip())

# Scrape jobs from Karkidi (fast version, no delay)
def scrape_karkidi_jobs(keyword="data science", pages=1):
    headers = {'User-Agent': 'Mozilla/5.0'}
    base_url = "https://www.karkidi.com/Find-Jobs/{page}/all/India?search={query}"
    jobs_list = []

    for page in range(1, pages + 1):
        print(f"Scraping page {page}...")
        url = base_url.format(page=page, query=keyword.replace(' ', '%20'))
        try:
            response = requests.get(url, headers=headers, timeout=10)
            soup = BeautifulSoup(response.content, "html.parser")
            job_blocks = soup.find_all("div", class_="ads-details")

            for job in job_blocks:
                title = (job.find("h4") or job.find("h2")).get_text(strip=True) if job.find("h4") or job.find("h2") else ""
                company_tag = job.find("a", href=lambda x: x and "Employer-Profile" in x)
                company = company_tag.get_text(strip=True) if company_tag else ""
                skills = ""
                key_skills_tag = job.find("span", string="Key Skills")
                if key_skills_tag:
                    skills = key_skills_tag.find_next("p").get_text(strip=True)
                jobs_list.append({"Title": title, "Company": company, "Skills": skills})
        except Exception as e:
            print(f"Error on page {page}: {e}")
            continue

    return pd.DataFrame(jobs_list)

# Classify jobs using pre-trained models
def classify_new_jobs(df):
    df["Cleaned_Skills"] = df["Skills"].apply(clean_skills)
    X = vectorizer.transform(df["Cleaned_Skills"])
    df["Predicted_Cluster"] = kmeans.predict(X)
    return df

# Run everything
def job_task():
    print("üîç Scraping job data...")
    df = scrape_karkidi_jobs(keyword="data science", pages=1)
    print(f" Scraped {len(df)} jobs.")

    print(" Classifying jobs...")
    df = classify_new_jobs(df)

    df.to_csv("daily_jobs_classified.csv", index=False)
    print(" Results saved to 'daily_jobs_classified.csv'")

# Run once
if __name__ == "__main__":
    job_task()
# At the end of job_automation.py

def run_automation():
    job_task()

if __name__ == "__main__":
    run_automation()
def run_automation(keyword="data science", pages=1):
    df = scrape_karkidi_jobs(keyword, pages)
    df = classify_new_jobs(df)
    df.to_csv("daily_jobs_classified.csv", index=False)


